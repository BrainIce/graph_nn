{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import join as pjoin\n",
    "import pickle\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 0.4.1\n"
     ]
    }
   ],
   "source": [
    "# Experiment parameters\n",
    "batch_size = 32\n",
    "threads = 0\n",
    "lr = 0.005\n",
    "epochs = 40\n",
    "log_interval = 10\n",
    "wdecay = 1e-4\n",
    "dataset = 'proteins'\n",
    "device = 'cpu'#'cuda'\n",
    "visualize = False\n",
    "shuffle_nodes = False\n",
    "n_folds = 10  # 10-fold cross validation\n",
    "seed = 111\n",
    "print('torch', torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GraphData(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 datareader,\n",
    "                 fold_id,\n",
    "                 split):\n",
    "        self.fold_id = fold_id\n",
    "        self.split = split\n",
    "        self.rnd_state = datareader.rnd_state\n",
    "        self.set_fold(datareader.data, fold_id)\n",
    "\n",
    "    def set_fold(self, data, fold_id):\n",
    "        self.total = len(data['targets'])\n",
    "        self.N_nodes_max = data['N_nodes_max']\n",
    "        self.n_classes = data['n_classes']\n",
    "        self.features_dim = data['features_dim']\n",
    "        self.idx = data['splits'][fold_id][self.split]\n",
    "         # use deepcopy to make sure we don't alter objects in folds\n",
    "        self.labels = copy.deepcopy([data['targets'][i] for i in self.idx])\n",
    "        self.adj_lists = copy.deepcopy([data['adj_list'][i] for i in self.idx])\n",
    "        self.signals_onehot = copy.deepcopy([data['signals_onehot'][i] for i in self.idx])\n",
    "        print(self.split, fold_id, len(self.labels), len(self.signals_onehot), len(data['targets']))\n",
    "        self.indices = np.arange(len(self.idx))  # sample indices for this epoch\n",
    "        \n",
    "    def pad(self, mtx, desired_dim1, desired_dim2=None, value=0):\n",
    "        sz = mtx.shape\n",
    "        assert len(sz) == 2, ('only 2d arrays are supported', sz)\n",
    "        # if np.all(np.array(sz) < desired_dim1 / 3): print('matrix shape is suspiciously small', sz, desired_dim1)\n",
    "        if desired_dim2 is not None:\n",
    "            mtx = np.pad(mtx, ((0, desired_dim1 - sz[0]), (0, desired_dim2 - sz[1])), 'constant', constant_values=value)\n",
    "        else:\n",
    "            mtx = np.pad(mtx, ((0, desired_dim1 - sz[0]), (0, 0)), 'constant', constant_values=value)\n",
    "        return mtx\n",
    "    \n",
    "    def nested_list_to_torch(self, data):\n",
    "        if isinstance(data, dict):\n",
    "            keys = list(data.keys())           \n",
    "        for i in range(len(data)):\n",
    "            if isinstance(data, dict):\n",
    "                i = keys[i]\n",
    "            if isinstance(data[i], np.ndarray):\n",
    "                data[i] = torch.from_numpy(data[i]).float()\n",
    "            elif isinstance(data[i], list):\n",
    "                data[i] = list_to_torch(data[i])\n",
    "        return data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = self.indices[index]\n",
    "        N_nodes_max = self.N_nodes_max\n",
    "        N_nodes = self.adj_lists[index].shape[0]\n",
    "        graph_support = np.zeros(self.N_nodes_max)\n",
    "        graph_support[:N_nodes] = 1\n",
    "        return self.nested_list_to_torch([self.pad(self.signals_onehot[index].copy(), self.N_nodes_max),  # node_features\n",
    "                                          self.pad(self.adj_lists[index], self.N_nodes_max, self.N_nodes_max),  # adjacency matrix\n",
    "                                          graph_support,  # mask with values of 0 for dummy (zero padded) nodes, otherwise 1 \n",
    "                                          N_nodes,\n",
    "                                          int(self.labels[index])])  # convert to torch\n",
    "\n",
    "\n",
    "class DataReader():\n",
    "    '''\n",
    "    Class to read the txt files containing all data of the dataset\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 data_dir,  # folder with txt files\n",
    "                 rnd_state=None,\n",
    "                 use_cont_node_attr=False,  # use or not additional float valued node attributes available in some datasets\n",
    "                 folds=10):\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.rnd_state = np.random.RandomState() if rnd_state is None else rnd_state\n",
    "        self.use_cont_node_attr = use_cont_node_attr\n",
    "        files = os.listdir(self.data_dir)\n",
    "        data = {}\n",
    "        nodes, graphs = self.read_graph_nodes_relations(list(filter(lambda f: f.find('graph_indicator') >= 0, files))[0])\n",
    "        data['labels'] = self.read_node_features(list(filter(lambda f: f.find('node_labels') >= 0, files))[0], \n",
    "                                                 nodes, graphs, fn=lambda s: int(s.strip()))  \n",
    "        data['adj_list'] = self.read_graph_adj(list(filter(lambda f: f.find('_A') >= 0, files))[0], nodes, graphs)\n",
    "                      \n",
    "        with open(pjoin(data_dir, list(filter(lambda f: f.find('graph_labels') >= 0, files))[0]), 'r') as fp:\n",
    "            lines = fp.readlines()\n",
    "        data['targets'] = np.array([int(float(l.strip())) for i, l in enumerate(lines)])\n",
    "        \n",
    "        if self.use_cont_node_attr:\n",
    "            data['attr'] = self.read_node_features(list(filter(lambda f: f.find('node_attributes') >= 0, files))[0], \n",
    "                                                   nodes, graphs, fn=lambda s: np.array(list(map(float, s.strip().split(',')))))\n",
    "        \n",
    "        signals, n_edges, degrees = [], [], []\n",
    "        for sample_id, adj in enumerate(data['adj_list']):\n",
    "            N = len(adj)  # number of nodes\n",
    "            if data['labels'] is not None:\n",
    "                assert N == len(data['labels'][sample_id]), (N, len(data['labels'][sample_id]))\n",
    "            n = np.sum(adj)  # undirected edges, so need to divide by 2\n",
    "            assert n % 2 == 0, n\n",
    "            n_edges.append( int(n / 2) )\n",
    "            if not np.allclose(adj, adj.T):\n",
    "                print(sample_id, 'not symmetric')\n",
    "            degrees.extend(list(np.sum(adj, 1)))\n",
    "            signal = np.array(data['labels'][sample_id]) # list(map(int, data['labels'][sample_id]))\n",
    "            signals.append(signal)\n",
    "            \n",
    "            \n",
    "        shapes = [len(adj) for adj in data['adj_list']]\n",
    "        # Create signals over graphs as one-hot vectors\n",
    "        m1 = np.min([np.min(signal) for signal in signals])\n",
    "        m2 = np.max([np.max(signal) for signal in signals])\n",
    "        features_dim = int(m2 - m1 + 1)  # number of possible values\n",
    "        signals_onehot = []\n",
    "        signal_arr = np.concatenate(signals)\n",
    "        \n",
    "        for i, signal in enumerate(signals):\n",
    "            signal_onehot = np.zeros((len(signal), features_dim))\n",
    "            for node, value in enumerate(signal):\n",
    "                signal_onehot[node, value - m1] = 1\n",
    "            if self.use_cont_node_attr:\n",
    "#                 print(signal_onehot.shape, np.array(data['attr'][i]).shape)\n",
    "                signal_onehot = np.concatenate((signal_onehot, np.array(data['attr'][i])), axis=1)\n",
    "            signals_onehot.append(signal_onehot)\n",
    "\n",
    "        if self.use_cont_node_attr:\n",
    "            features_dim = signals_onehot[0].shape[1]\n",
    "            \n",
    "        labels = data['targets']        # graph class labels\n",
    "        labels -= np.min(labels)        # to start from 0\n",
    "        N_nodes_max = np.max(shapes)    # max number of nodes\n",
    "\n",
    "        classes = np.unique(labels)\n",
    "        n_classes = len(classes)\n",
    "\n",
    "        if classes[-1] != n_classes - 1:\n",
    "            print('making labels sequential, otherwise pytorch crashes')\n",
    "            labels_new = np.zeros(labels.shape, dtype=labels.dtype) - 1\n",
    "            for lbl in range(n_classes):\n",
    "                labels_new[labels == classes[lbl]] = lbl\n",
    "            labels = labels_new\n",
    "            classes = np.unique(labels)\n",
    "            assert len(np.unique(labels)) == n_classes, np.unique(labels)\n",
    "\n",
    "        print('N nodes avg/std/min/max: \\t%.2f/%.2f/%d/%d' % (np.mean(shapes), np.std(shapes), np.min(shapes), N_nodes_max))\n",
    "        print('N edges avg/std/min/max: \\t%.2f/%.2f/%d/%d' % (np.mean(n_edges), np.std(n_edges), np.min(n_edges), np.max(n_edges)))\n",
    "        print('Node degree avg/std/min/max: \\t%.2f/%.2f/%d/%d' % (np.mean(degrees), np.std(degrees), np.min(degrees), np.max(degrees)))\n",
    "        print('Node features dim: \\t\\t%d' % features_dim)\n",
    "        print('N classes: \\t\\t\\t%d' % n_classes)\n",
    "        print('Classes: \\t\\t\\t%s' % str(classes))\n",
    "        for lbl in classes:\n",
    "            print('Class %d: \\t\\t\\t%d samples' % (lbl, np.sum(labels == lbl)))\n",
    "\n",
    "        for u in np.unique(signal_arr):\n",
    "            print('feature {}, count {}/{}'.format(u, np.count_nonzero(signal_arr == u), len(signal_arr)))\n",
    "        \n",
    "        N_graphs = len(labels)  # number of samples (graphs) in data\n",
    "        assert N_graphs == len(data['adj_list']) == len(signals_onehot), 'invalid data'\n",
    "\n",
    "        # Create test sets first\n",
    "        train_ids, test_ids = self.split_ids(np.arange(N_graphs), rnd_state=self.rnd_state, folds=folds)\n",
    "\n",
    "        # Create train sets\n",
    "        splits = []\n",
    "        for fold in range(folds):\n",
    "            splits.append({'train': train_ids[fold],\n",
    "                           'test': test_ids[fold]})\n",
    "\n",
    "        data['signals_onehot'] = signals_onehot\n",
    "        data['targets'] = labels\n",
    "        data['splits'] = splits \n",
    "        data['N_nodes_max'] = N_nodes_max\n",
    "        data['features_dim'] = features_dim\n",
    "        data['n_classes'] = n_classes\n",
    "        \n",
    "        self.data = data\n",
    "\n",
    "    def split_ids(self, ids_all, rnd_state=None, folds=10):\n",
    "        n = len(ids_all)\n",
    "        ids = ids_all[rnd_state.permutation(n)]\n",
    "        stride = int(np.ceil(n / float(folds)))\n",
    "        test_ids = [ids[i: i + stride] for i in range(0, n, stride)]\n",
    "        assert np.all(np.unique(np.concatenate(test_ids)) == sorted(ids_all)), 'some graphs are missing in the test sets'\n",
    "        assert len(test_ids) == folds, 'invalid test sets'\n",
    "        train_ids = []\n",
    "        for fold in range(folds):\n",
    "            train_ids.append(np.array([e for e in ids if e not in test_ids[fold]]))\n",
    "            assert len(train_ids[fold]) + len(test_ids[fold]) == len(np.unique(list(train_ids[fold]) + list(test_ids[fold]))) == n, 'invalid splits'\n",
    "\n",
    "        return train_ids, test_ids\n",
    "\n",
    "    def parse_txt_file(self, fpath, line_parse_fn=None):\n",
    "        with open(pjoin(self.data_dir, fpath), 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        data = [line_parse_fn(s) if line_parse_fn is not None else s for s in lines]\n",
    "        return data\n",
    "    \n",
    "    def read_graph_adj(self, fpath, nodes, graphs):\n",
    "        edges = self.parse_txt_file(fpath, line_parse_fn=lambda s: s.split(','))\n",
    "        adj_dict = {}\n",
    "        for edge in edges:\n",
    "            node1 = int(edge[0].strip()) - 1  # -1 because of zero-indexing in our code\n",
    "            node2 = int(edge[1].strip()) - 1\n",
    "            graph_id = nodes[node1]\n",
    "            assert graph_id == nodes[node2], ('invalid data', graph_id, nodes[node2])\n",
    "            if graph_id not in adj_dict:\n",
    "                n = len(graphs[graph_id])\n",
    "                adj_dict[graph_id] = np.zeros((n, n))\n",
    "            ind1 = np.where(graphs[graph_id] == node1)[0]\n",
    "            ind2 = np.where(graphs[graph_id] == node2)[0]\n",
    "            assert len(ind1) == len(ind2) == 1, (ind1, ind2)\n",
    "            adj_dict[graph_id][ind1, ind2] = 1\n",
    "            \n",
    "        adj_list = [adj_dict[graph_id] for graph_id in sorted(list(graphs.keys()))]\n",
    "        \n",
    "        return adj_list\n",
    "        \n",
    "    def read_graph_nodes_relations(self, fpath):\n",
    "        graph_ids = self.parse_txt_file(fpath, line_parse_fn=lambda s: int(s.rstrip()))\n",
    "        nodes, graphs = {}, {}\n",
    "        for node_id, graph_id in enumerate(graph_ids):\n",
    "            if graph_id not in graphs:\n",
    "                graphs[graph_id] = []\n",
    "            graphs[graph_id].append(node_id)\n",
    "            nodes[node_id] = graph_id\n",
    "        graph_ids = np.unique(list(graphs.keys()))\n",
    "        for graph_id in graphs:\n",
    "            graphs[graph_id] = np.array(graphs[graph_id])\n",
    "        return nodes, graphs\n",
    "\n",
    "    def read_node_features(self, fpath, nodes, graphs, fn):\n",
    "        node_features_all = self.parse_txt_file(fpath, line_parse_fn=fn)\n",
    "        node_features = {}\n",
    "        for node_id, x in enumerate(node_features_all):\n",
    "            graph_id = nodes[node_id]\n",
    "            if graph_id not in node_features:\n",
    "                node_features[graph_id] = [ None ] * len(graphs[graph_id])\n",
    "            ind = np.where(graphs[graph_id] == node_id)[0]\n",
    "            assert len(ind) == 1, ind\n",
    "            assert node_features[graph_id][ind[0]] is None, node_features[graph_id][ind[0]]\n",
    "            node_features[graph_id][ind[0]] = x\n",
    "        node_features_lst = [node_features[graph_id] for graph_id in sorted(list(graphs.keys()))]\n",
    "        return node_features_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                in_features,\n",
    "                out_features,\n",
    "                activation=None,\n",
    "                adj_sq=False,\n",
    "                scale_identity=False):\n",
    "        super(GraphConv, self).__init__()\n",
    "        self.fc = nn.Linear(in_features=in_features, out_features=out_features)\n",
    "        self.adj_sq = adj_sq\n",
    "        self.activation = activation\n",
    "        self.scale_identity = scale_identity\n",
    "            \n",
    "    def laplacians_batch(self, W):\n",
    "        batch, N = W.shape[:2]\n",
    "        if self.adj_sq:\n",
    "            W = torch.bmm(W, W)\n",
    "        W_hat = W + (int(self.scale_identity) + 1) * torch.eye(N).unsqueeze(0).to(device)\n",
    "        D_hat = (torch.sum(W_hat, 1) + 1e-5) ** (-0.5)\n",
    "        L = D_hat.view(batch, N, 1) * W_hat * D_hat.view(batch, 1, N)\n",
    "        return L\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, W = data[:2]\n",
    "        x = self.fc(torch.bmm(self.laplacians_batch(W), x))\n",
    "        if self.activation is not None:\n",
    "            x = self.activation(x)\n",
    "        return (x, W)\n",
    "        \n",
    "class GCN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 out_features,\n",
    "                 filters=[32,32,32],\n",
    "                 n_hidden=96,\n",
    "                 dropout=0.5,\n",
    "                 adj_sq=False,\n",
    "                 scale_identity=False):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        # Graph convolution layers\n",
    "        self.gconv = nn.Sequential(*([GraphConv(in_features=in_features if layer == 0 else filters[layer - 1], \n",
    "                                                out_features=f, \n",
    "                                                activation=nn.ReLU(inplace=True),\n",
    "                                               adj_sq=adj_sq,\n",
    "                                               scale_identity=scale_identity) for layer, f in enumerate(filters)]))\n",
    "        \n",
    "        # Fully connected layers\n",
    "        fc = []\n",
    "        if dropout > 0:\n",
    "            fc.append(nn.Dropout(p=dropout))\n",
    "        if n_hidden > 0:\n",
    "            fc.append(nn.Linear(filters[-1], n_hidden))\n",
    "            if dropout > 0:\n",
    "                fc.append(nn.Dropout(p=dropout))\n",
    "            n_last = n_hidden\n",
    "        else:\n",
    "            n_last = filters[-1]\n",
    "        fc.append(nn.Linear(n_last, out_features))       \n",
    "        self.fc = nn.Sequential(*fc)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x = self.gconv(data)[0]\n",
    "        x = torch.max(x, dim=1)[0].squeeze()  # max pooling over nodes\n",
    "        x = self.fc(x)\n",
    "        return x  \n",
    "    \n",
    "class GraphUnet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 out_features,\n",
    "                 filters=[32,32,32],\n",
    "                 n_hidden=96,\n",
    "                 dropout=0.5,\n",
    "                 adj_sq=False,\n",
    "                 scale_identity=False,\n",
    "                 shuffle_nodes=False,\n",
    "                 visualize=False,\n",
    "                 pooling_ratios=[0.8, 0.8]):\n",
    "        super(GraphUnet, self).__init__()\n",
    "\n",
    "        self.shuffle_nodes = shuffle_nodes\n",
    "        self.visualize = visualize\n",
    "        self.pooling_ratios = pooling_ratios\n",
    "        # Graph convolution layers\n",
    "        self.gconv = nn.ModuleList([GraphConv(in_features=in_features if layer == 0 else filters[layer - 1], \n",
    "                                                out_features=f, \n",
    "                                                activation=nn.ReLU(inplace=True),\n",
    "                                               adj_sq=adj_sq,\n",
    "                                               scale_identity=scale_identity) for layer, f in enumerate(filters)])\n",
    "        self.proj = []\n",
    "        for layer, f in enumerate(filters[:-1]):\n",
    "            fan_in = filters[layer]\n",
    "            p = Parameter(torch.Tensor(fan_in, 1))\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            torch.nn.init.uniform_(p, -bound, bound)\n",
    "            self.proj.append(p)\n",
    "#         self.proj = nn.ModuleList(proj)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        fc = []\n",
    "        if dropout > 0:\n",
    "            fc.append(nn.Dropout(p=dropout))\n",
    "        if n_hidden > 0:\n",
    "            fc.append(nn.Linear(filters[-1], n_hidden))\n",
    "            if dropout > 0:\n",
    "                fc.append(nn.Dropout(p=dropout))\n",
    "            n_last = n_hidden\n",
    "        else:\n",
    "            n_last = filters[-1]\n",
    "        fc.append(nn.Linear(n_last, out_features))       \n",
    "        self.fc = nn.Sequential(*fc)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        # [signal, W, signal_support, N_nodes, int(label)]\n",
    "        if self.shuffle_nodes:\n",
    "            N = data[0].shape[1]\n",
    "            idx = torch.randperm(N)\n",
    "            data = (data[0][:, idx], data[1][:, idx][idx, :], data[2][:, idx], data[3])\n",
    "        plot = -1\n",
    "        N_nodes_tmp = -1\n",
    "        for layer, gconv in enumerate(self.gconv):\n",
    "            N_nodes = data[3]\n",
    "            N_nodes_max = N_nodes.max()\n",
    "            #print('layer', layer, N_nodes_max)\n",
    "            #data = (data[0][:, :N_nodes_max], data[1][:, :N_nodes_max, :N_nodes_max], data[2][:, :N_nodes_max], data[3])      \n",
    "            B, N, _ = data[0].shape\n",
    "            if layer < len(self.gconv) - 1 and self.visualize:      \n",
    "                x, W = data[:2]\n",
    "                for b in range(B):\n",
    "                    if (layer == 0 and N_nodes[b] < 20 and N_nodes[b] > 10) or plot > -1:\n",
    "                        if plot > -1 and plot != b:\n",
    "                            continue\n",
    "                        if N_nodes_tmp < 0:\n",
    "                            N_nodes_tmp = N_nodes[b]\n",
    "                        plt.figure(figsize=(18,5))\n",
    "                        plt.subplot(141)\n",
    "                        plt.title('layer %d, Input adjacency matrix' % (layer))\n",
    "                        plt.imshow(W[b][:N_nodes_tmp, :N_nodes_tmp].data.cpu().numpy())\n",
    "                        plot = b                        \n",
    "                        break\n",
    "            mask = data[2].clone()\n",
    "            data = gconv(data)\n",
    "            x, W = data\n",
    "            if layer < len(self.gconv) - 1:\n",
    "                B, N, C = x.shape\n",
    "                y = torch.mm(x.view(B * N, C), self.proj[layer]).view(B, N)\n",
    "                y = y / (torch.sum(self.proj[layer] ** 2).view(1, 1) ** 0.5)  # node scores used for ranking below\n",
    "                #y[mask == 0] = y.min() - 0.1\n",
    "                idx = torch.sort(y, dim=1)[1]  # B,N                \n",
    "                N_remove = (N_nodes.float() * (1 - self.pooling_ratios[layer])).long()\n",
    "                assert torch.all(N_nodes > N_remove), 'the number of removed nodes must be large than the number of nodes'\n",
    "                for b in range(B):\n",
    "                    assert torch.sum(mask[b]) == float(N_nodes[b]), (torch.sum(mask[b]), N_nodes[b])\n",
    "                N_nodes_prev = N_nodes\n",
    "                N_nodes = N_nodes - N_remove\n",
    "                                \n",
    "                for b in range(B):\n",
    "                    idx_b = idx[b, mask[b, idx[b]] == 1]\n",
    "                    assert len(idx_b) >= N_nodes[b], (len(idx_b), N_nodes[b])\n",
    "                    mask[b, idx_b[:N_remove[b]]] = 0\n",
    "                for b in range(B):\n",
    "                    assert torch.sum(mask[b]) == float(N_nodes[b]), (b, torch.sum(mask[b]), N_nodes[b], N_remove[b], N_nodes_prev[b])\n",
    "                    s = torch.sum(y[b] >= torch.min((y * mask.float())[b]))\n",
    "                    assert s >= float(N_nodes[b]), (s, N_nodes[b], (y * mask.float())[b])\n",
    "                \n",
    "                mask = mask.unsqueeze(2)\n",
    "                x = x * torch.tanh(y).unsqueeze(2) * mask\n",
    "                W = mask * W * mask.view(B, 1, N)\n",
    "                mask = mask.squeeze()\n",
    "                data = (x, W, mask, N_nodes)\n",
    "                \n",
    "                if self.visualize:\n",
    "                    for b in range(B):\n",
    "                        if plot == b:\n",
    "                            plt.subplot(142)\n",
    "                            plt.title('layer %d, Ranking' % (layer))\n",
    "                            plt.imshow(y[b].view(N, 1).expand(N, 2)[:N_nodes_tmp].data.cpu().numpy())\n",
    "                            plt.colorbar()\n",
    "                            plt.subplot(143)\n",
    "                            plt.title('layer %d, Pooled nodes' % (layer))\n",
    "                            plt.imshow(mask[b].view(N, 1).expand(N, 2)[:N_nodes_tmp].data.cpu().numpy())\n",
    "                            plt.subplot(144)\n",
    "                            plt.title('layer %d, Pooled adjacency matrix' % (layer))\n",
    "                            plt.imshow(W[b][:N_nodes_tmp, :N_nodes_tmp].data.cpu().numpy())\n",
    "                            plt.show()\n",
    "                        \n",
    "        if self.visualize and plot > -1:\n",
    "            raise ValueError('stop and see visualizations')\n",
    "        x = torch.max(x, dim=1)[0].squeeze()  # max pooling over nodes\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N nodes avg/std/min/max: \t39.06/45.76/4/620\n",
      "N edges avg/std/min/max: \t72.82/84.60/5/1049\n",
      "Node degree avg/std/min/max: \t3.73/1.15/0/25\n",
      "Node features dim: \t\t4\n",
      "N classes: \t\t\t2\n",
      "Classes: \t\t\t[0 1]\n",
      "Class 0: \t\t\t663 samples\n",
      "Class 1: \t\t\t450 samples\n",
      "feature 0, count 21151/43471\n",
      "feature 1, count 20931/43471\n",
      "feature 2, count 1389/43471\n",
      "fold 0\n",
      "train 0 1001 1001 1113\n",
      "test 0 112 112 1113\n",
      "GraphUnet(\n",
      "  (gconv): ModuleList(\n",
      "    (0): GraphConv(\n",
      "      (fc): Linear(in_features=4, out_features=64, bias=True)\n",
      "      (activation): ReLU(inplace)\n",
      "    )\n",
      "    (1): GraphConv(\n",
      "      (fc): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (activation): ReLU(inplace)\n",
      "    )\n",
      "    (2): GraphConv(\n",
      "      (fc): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (activation): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "N params 8770\n",
      "Train Epoch: 0 [32/1001 (3%)]\tLoss: 0.688701 (avg: 0.688701) \tsec/iter: 0.5914\n",
      "Train Epoch: 0 [352/1001 (34%)]\tLoss: 0.785490 (avg: 0.682564) \tsec/iter: 0.6115\n",
      "Train Epoch: 0 [672/1001 (66%)]\tLoss: 0.752477 (avg: 0.678905) \tsec/iter: 0.6169\n",
      "Train Epoch: 0 [992/1001 (97%)]\tLoss: 0.655887 (avg: 0.681640) \tsec/iter: 0.6179\n",
      "Train Epoch: 0 [1001/1001 (100%)]\tLoss: 0.659526 (avg: 0.681441) \tsec/iter: 0.6046\n",
      "Test set (epoch 0): Average loss: 0.6694, Accuracy: 597/1001 (59.64%)\n",
      "\n",
      "Train Epoch: 1 [32/1001 (3%)]\tLoss: 0.692594 (avg: 0.692594) \tsec/iter: 0.6665\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-119-62a487876c7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[0mloss_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloaders\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloaders\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[0macc_folds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-119-62a487876c7a>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_loader)\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-118-0284029db0a8>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    167\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m                 \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mW\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_nodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "datareader = DataReader(data_dir='./data/%s/' % dataset.upper(),\n",
    "                        rnd_state=np.random.RandomState(seed),\n",
    "                        folds=n_folds,                    \n",
    "                        use_cont_node_attr=True)\n",
    "\n",
    "acc_folds = []\n",
    "for fold_id in range(n_folds):\n",
    "    print('fold', fold_id)\n",
    "    loaders = []\n",
    "    for split in ['train', 'test']:\n",
    "        gdata = GraphData(fold_id=fold_id,\n",
    "                             datareader=datareader,\n",
    "                             split=split)\n",
    "\n",
    "        loader = torch.utils.data.DataLoader(gdata, \n",
    "                                             batch_size=batch_size,\n",
    "                    #                                            collate_fn=graphs_collate_fn,\n",
    "                                           shuffle=split.find('train') >= 0, # for optimized code we have a built-in shuffler to get info about graph sizes in batches\n",
    "                                           num_workers=threads)\n",
    "        loaders.append(loader)\n",
    "    \n",
    "    model = GraphUnet(in_features=loaders[0].dataset.features_dim,\n",
    "                      out_features=loaders[0].dataset.n_classes,\n",
    "                      n_hidden=0,\n",
    "                      filters=[64,64,64],\n",
    "                      dropout=0,\n",
    "                      adj_sq=False,\n",
    "                      scale_identity=False,\n",
    "                      shuffle_nodes=shuffle_nodes,\n",
    "                      visualize=visualize).to(device)\n",
    "    print(model)\n",
    "    c = 0\n",
    "    for p in filter(lambda p: p.requires_grad, model.parameters()):\n",
    "        c += p.numel()\n",
    "    print('N params', c)\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "                filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                lr=lr,\n",
    "                weight_decay=wdecay,\n",
    "                betas=(0.5, 0.999))\n",
    "    #         scheduler = lr_scheduler.MultiStepLR(optimizer, args.lr_decay_step, gamma=0.1)\n",
    "\n",
    "    def train(train_loader):\n",
    "        model.train()\n",
    "        start = time.time()\n",
    "        train_loss, n_samples = 0, 0\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            for i in range(len(data)):\n",
    "                data[i] = data[i].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, data[4])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            time_iter = time.time() - start\n",
    "            train_loss += loss.item() * len(output)\n",
    "            n_samples += len(output)\n",
    "            if batch_idx % log_interval == 0 or batch_idx == len(train_loader) - 1:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} (avg: {:.6f}) \\tsec/iter: {:.4f}'.format(\n",
    "                    epoch, n_samples, len(train_loader.dataset),\n",
    "                    100. * (batch_idx + 1) / len(train_loader), loss.item(), train_loss / n_samples, time_iter / (batch_idx + 1) ))\n",
    "    #             break \n",
    "    def test(test_loader):\n",
    "        model.eval()\n",
    "        start = time.time()\n",
    "        test_loss, correct, n_samples = 0, 0, 0\n",
    "        for batch_idx, data in enumerate(test_loader):\n",
    "            for d in data:\n",
    "                d = d.to(device)\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, data[4], reduction='sum')\n",
    "            test_loss += loss.item()\n",
    "            n_samples += len(output)\n",
    "            pred = output.detach().cpu().max(1, keepdim=True)[1]\n",
    "\n",
    "            correct += pred.eq(data[4].detach().cpu().view_as(pred)).sum().item()\n",
    "\n",
    "        time_iter = time.time() - start\n",
    "\n",
    "        test_loss /= n_samples\n",
    "\n",
    "        acc = 100. * correct / n_samples\n",
    "        print('Test set (epoch {}): Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(epoch, \n",
    "                                                                                              test_loss, \n",
    "                                                                                              correct, \n",
    "                                                                                              n_samples, acc))\n",
    "        return acc\n",
    "\n",
    "    loss_fn = F.cross_entropy\n",
    "    for epoch in range(epochs):\n",
    "        train(loaders[0])\n",
    "        acc = test(loaders[0])\n",
    "    acc_folds.append(acc)\n",
    "\n",
    "print(acc_folds)\n",
    "print('{}-fold cross validation avg acc (+- std): {} ({})'.format(n_folds, np.mean(acc_folds), np.std(acc_folds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
